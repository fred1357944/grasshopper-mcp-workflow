"""
Elegance Metrics - å„ªé›…åº¦è©•ä¼°æ¨¡çµ„
==================================
é‡åŒ–è©•ä¼° Grasshopper æ–¹æ¡ˆçš„å„ªé›…ç¨‹åº¦
"""

from dataclasses import dataclass, field
from typing import Dict, List, Any, Tuple, Optional
from enum import Enum


class MetricCategory(Enum):
    """è©•ä¼°æŒ‡æ¨™é¡åˆ¥"""
    SIMPLICITY = "simplicity"           # ç°¡æ½”æ€§
    EFFICIENCY = "efficiency"           # æ•ˆç‡
    FLEXIBILITY = "flexibility"         # éˆæ´»æ€§
    MAINTAINABILITY = "maintainability" # å¯ç¶­è­·æ€§
    ELEGANCE = "elegance"               # å„ªé›…åº¦


@dataclass
class MetricResult:
    """å–®é …æŒ‡æ¨™è©•ä¼°çµæœ"""
    name: str
    category: MetricCategory
    score: float  # 0-1
    weight: float
    description: str
    issues: List[str] = field(default_factory=list)
    suggestions: List[str] = field(default_factory=list)


@dataclass
class EleganceReport:
    """å®Œæ•´çš„å„ªé›…åº¦è©•ä¼°å ±å‘Š"""
    total_score: float
    metrics: List[MetricResult]
    summary: str
    grade: str  # A, B, C, D, F
    
    @property
    def weighted_score(self) -> float:
        """è¨ˆç®—åŠ æ¬Šç¸½åˆ†"""
        total_weight = sum(m.weight for m in self.metrics)
        if total_weight == 0:
            return 0
        return sum(m.score * m.weight for m in self.metrics) / total_weight
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "total_score": self.total_score,
            "grade": self.grade,
            "summary": self.summary,
            "metrics": [
                {
                    "name": m.name,
                    "category": m.category.value,
                    "score": m.score,
                    "weight": m.weight,
                    "description": m.description,
                    "issues": m.issues,
                    "suggestions": m.suggestions
                }
                for m in self.metrics
            ]
        }


# ============================================================
# å„ªé›…å…ƒä»¶èˆ‡åæ¨¡å¼å®šç¾©
# ============================================================

ELEGANT_COMPONENTS = {
    # å„ªé›…çš„æ•¸æ“šè™•ç†å…ƒä»¶
    "Graph Mapper": {"score_bonus": 0.15, "reason": "éç·šæ€§æ•¸æ“šæ˜ å°„"},
    "Remap Numbers": {"score_bonus": 0.10, "reason": "å‹•æ…‹ç¯„åœæ˜ å°„"},
    "Expression": {"score_bonus": 0.12, "reason": "å–®å…ƒä»¶å¤šåŠŸèƒ½"},
    "Evaluate Surface": {"score_bonus": 0.10, "reason": "å¹¾ä½•é©…å‹•åƒæ•¸"},
    "Evaluate Curve": {"score_bonus": 0.10, "reason": "å¹¾ä½•é©…å‹•åƒæ•¸"},
    "Closest Point": {"score_bonus": 0.08, "reason": "å¹¾ä½•é—œè¯"},
    "Surface CP": {"score_bonus": 0.08, "reason": "å¹¾ä½•é©…å‹•"},
    "Bounds": {"score_bonus": 0.05, "reason": "å‹•æ…‹ç¯„åœ"},
}

ANTI_PATTERN_COMPONENTS = {
    # éåº¦ä½¿ç”¨æœƒé™ä½åˆ†æ•¸çš„å…ƒä»¶
    "Relay": {"penalty_per_excess": 0.02, "threshold": 5},
    "Panel": {"penalty_per_excess": 0.01, "threshold": 3},
    "Flatten": {"penalty_per_excess": 0.03, "threshold": 3},
    "Graft": {"penalty_per_excess": 0.03, "threshold": 3},
}

ANTI_PATTERNS = [
    {
        "name": "slider_explosion",
        "description": "Slider æ•¸é‡éå¤š",
        "detector": lambda c: sum(1 for comp in c if "slider" in comp.lower()) > 8,
        "penalty": 0.15,
        "suggestion": "ä½¿ç”¨ Gene Pool æˆ– Expression åˆä½µç›¸é—œåƒæ•¸"
    },
    {
        "name": "datatree_chaos",
        "description": "DataTree æ“ä½œéæ–¼è¤‡é›œ",
        "detector": lambda c: sum(1 for comp in c if any(kw in comp.lower() for kw in ["flatten", "graft", "partition"])) > 5,
        "penalty": 0.10,
        "suggestion": "é‡æ–°è¨­è¨ˆæ•¸æ“šçµæ§‹ï¼Œæ¸›å°‘ DataTree æ“ä½œ"
    },
    {
        "name": "isolated_components",
        "description": "å­˜åœ¨å­¤ç«‹å…ƒä»¶",
        "detector": None,  # éœ€è¦é€£æ¥è³‡è¨Šï¼Œç”±å¤–éƒ¨æä¾›
        "penalty": 0.08,
        "suggestion": "ç§»é™¤æœªä½¿ç”¨çš„å…ƒä»¶æˆ–æª¢æŸ¥é€£æ¥"
    },
    {
        "name": "duplicate_sliders",
        "description": "å­˜åœ¨åŠŸèƒ½é‡è¤‡çš„ Slider",
        "detector": None,  # éœ€è¦æ›´è¤‡é›œçš„åˆ†æ
        "penalty": 0.05,
        "suggestion": "åˆä½µåŠŸèƒ½ç›¸ä¼¼çš„ Slider"
    },
]


class EleganceEvaluator:
    """
    å„ªé›…åº¦è©•ä¼°å™¨
    
    è©•ä¼° Grasshopper æ–¹æ¡ˆçš„å„é …æŒ‡æ¨™
    """
    
    def __init__(self):
        self.metrics_config = {
            # === ç°¡æ½”æ€§æŒ‡æ¨™ ===
            "slider_count": {
                "weight": 0.15,
                "category": MetricCategory.SIMPLICITY,
                "description": "æ§åˆ¶åƒæ•¸æ•¸é‡",
                "optimal_range": (1, 5),
                "penalty_rate": 0.1,
            },
            "component_count": {
                "weight": 0.10,
                "category": MetricCategory.SIMPLICITY,
                "description": "å…ƒä»¶ç¸½æ•¸æ•ˆç‡",
                "optimal_ratio": 0.8,  # é€£æ¥æ•¸/å…ƒä»¶æ•¸
            },
            
            # === æ•ˆç‡æŒ‡æ¨™ ===
            "connection_density": {
                "weight": 0.12,
                "category": MetricCategory.EFFICIENCY,
                "description": "é€£æ¥å¯†åº¦",
                "optimal_range": (0.7, 1.5),
            },
            "datatree_complexity": {
                "weight": 0.10,
                "category": MetricCategory.EFFICIENCY,
                "description": "DataTree æ“ä½œè¤‡é›œåº¦",
                "penalty_components": ["Flatten", "Graft", "Partition", "Path Mapper"],
            },
            
            # === éˆæ´»æ€§æŒ‡æ¨™ ===
            "parametric_depth": {
                "weight": 0.15,
                "category": MetricCategory.FLEXIBILITY,
                "description": "åƒæ•¸åŒ–æ·±åº¦",
            },
            "geometric_coupling": {
                "weight": 0.18,
                "category": MetricCategory.FLEXIBILITY,
                "description": "å¹¾ä½•é©…å‹•ç¨‹åº¦",
                "positive_components": ["Evaluate", "Closest Point", "Surface CP"],
            },
            
            # === å„ªé›…åº¦æŒ‡æ¨™ ===
            "pattern_usage": {
                "weight": 0.12,
                "category": MetricCategory.ELEGANCE,
                "description": "å„ªé›…æ¨¡å¼ä½¿ç”¨",
            },
            "anti_pattern_absence": {
                "weight": 0.08,
                "category": MetricCategory.ELEGANCE,
                "description": "åæ¨¡å¼é¿å…",
            },
        }
    
    def evaluate(self, gh_code: Dict[str, Any], patterns_matched: List[str] = None) -> EleganceReport:
        """
        åŸ·è¡Œå®Œæ•´è©•ä¼°
        
        Args:
            gh_code: GH Code å®šç¾©
            patterns_matched: å·²åŒ¹é…çš„è¨­è¨ˆæ¨¡å¼
            
        Returns:
            EleganceReport è©•ä¼°å ±å‘Š
        """
        components = gh_code.get("components", [])
        connections = gh_code.get("connections", [])
        sliders = gh_code.get("sliders", [])
        
        component_names = [c.get("type", c.get("name", "")).lower() for c in components]
        
        metrics = []
        
        # 1. è©•ä¼° Slider æ•¸é‡
        metrics.append(self._evaluate_slider_count(sliders))
        
        # 2. è©•ä¼°å…ƒä»¶æ•¸é‡æ•ˆç‡
        metrics.append(self._evaluate_component_efficiency(components, connections))
        
        # 3. è©•ä¼°é€£æ¥å¯†åº¦
        metrics.append(self._evaluate_connection_density(components, connections))
        
        # 4. è©•ä¼° DataTree è¤‡é›œåº¦
        metrics.append(self._evaluate_datatree_complexity(component_names))
        
        # 5. è©•ä¼°å¹¾ä½•è€¦åˆåº¦
        metrics.append(self._evaluate_geometric_coupling(component_names))
        
        # 6. è©•ä¼°æ¨¡å¼ä½¿ç”¨
        metrics.append(self._evaluate_pattern_usage(component_names, patterns_matched or []))
        
        # 7. è©•ä¼°åæ¨¡å¼
        metrics.append(self._evaluate_anti_patterns(component_names, components, connections))
        
        # è¨ˆç®—ç¸½åˆ†
        total_weight = sum(m.weight for m in metrics)
        total_score = sum(m.score * m.weight for m in metrics) / total_weight if total_weight > 0 else 0
        
        # ç”Ÿæˆç­‰ç´š
        grade = self._score_to_grade(total_score)
        
        # ç”Ÿæˆæ‘˜è¦
        summary = self._generate_summary(metrics, total_score, grade)
        
        return EleganceReport(
            total_score=round(total_score, 3),
            metrics=metrics,
            summary=summary,
            grade=grade
        )
    
    def _evaluate_slider_count(self, sliders: List[Dict]) -> MetricResult:
        """è©•ä¼° Slider æ•¸é‡"""
        count = len(sliders)
        config = self.metrics_config["slider_count"]
        
        optimal_min, optimal_max = config["optimal_range"]
        
        if count == 0:
            score = 0.5
            issues = ["æ²’æœ‰åƒæ•¸åŒ–æ§åˆ¶"]
            suggestions = ["æ·»åŠ  Slider ä»¥å¯¦ç¾åƒæ•¸åŒ–"]
        elif count <= optimal_max:
            score = 1.0
            issues = []
            suggestions = []
        else:
            excess = count - optimal_max
            score = max(0.3, 1.0 - excess * config["penalty_rate"])
            issues = [f"Slider æ•¸é‡ ({count}) è¶…éå»ºè­°å€¼ ({optimal_max})"]
            suggestions = ["è€ƒæ…®ä½¿ç”¨ Gene Pool æˆ– Expression åˆä½µåƒæ•¸"]
        
        return MetricResult(
            name="Slider æ•¸é‡",
            category=config["category"],
            score=score,
            weight=config["weight"],
            description=f"ç›®å‰ {count} å€‹ Slider",
            issues=issues,
            suggestions=suggestions
        )
    
    def _evaluate_component_efficiency(
        self, 
        components: List[Dict], 
        connections: List[Dict]
    ) -> MetricResult:
        """è©•ä¼°å…ƒä»¶æ•ˆç‡"""
        config = self.metrics_config["component_count"]
        
        comp_count = len(components)
        conn_count = len(connections)
        
        if comp_count == 0:
            return MetricResult(
                name="å…ƒä»¶æ•ˆç‡",
                category=config["category"],
                score=0.0,
                weight=config["weight"],
                description="æ²’æœ‰å…ƒä»¶",
                issues=["æ–¹æ¡ˆç‚ºç©º"],
                suggestions=["éœ€è¦ç”Ÿæˆå…ƒä»¶"]
            )
        
        ratio = conn_count / comp_count
        optimal = config["optimal_ratio"]
        
        if ratio >= optimal:
            score = min(1.0, 0.7 + ratio * 0.3)
        else:
            score = max(0.4, ratio / optimal)
        
        issues = []
        suggestions = []
        
        if ratio < 0.5:
            issues.append("é€£æ¥å¯†åº¦è¼ƒä½ï¼Œå¯èƒ½å­˜åœ¨å­¤ç«‹å…ƒä»¶")
            suggestions.append("æª¢æŸ¥ä¸¦ç§»é™¤æœªé€£æ¥çš„å…ƒä»¶")
        
        return MetricResult(
            name="å…ƒä»¶æ•ˆç‡",
            category=config["category"],
            score=score,
            weight=config["weight"],
            description=f"{comp_count} å…ƒä»¶, {conn_count} é€£æ¥ (æ¯”ç‡: {ratio:.2f})",
            issues=issues,
            suggestions=suggestions
        )
    
    def _evaluate_connection_density(
        self, 
        components: List[Dict], 
        connections: List[Dict]
    ) -> MetricResult:
        """è©•ä¼°é€£æ¥å¯†åº¦"""
        config = self.metrics_config["connection_density"]
        
        comp_count = len(components)
        conn_count = len(connections)
        
        if comp_count <= 1:
            density = 0
        else:
            # ç†æƒ³å¯†åº¦ï¼šæ¯å€‹å…ƒä»¶å¹³å‡æœ‰ 1-2 å€‹é€£æ¥
            density = conn_count / max(1, comp_count - 1)
        
        optimal_min, optimal_max = config["optimal_range"]
        
        if optimal_min <= density <= optimal_max:
            score = 1.0
        elif density < optimal_min:
            score = max(0.4, density / optimal_min)
        else:
            score = max(0.6, 1.0 - (density - optimal_max) * 0.1)
        
        return MetricResult(
            name="é€£æ¥å¯†åº¦",
            category=config["category"],
            score=score,
            weight=config["weight"],
            description=f"é€£æ¥å¯†åº¦: {density:.2f}",
            issues=[],
            suggestions=[]
        )
    
    def _evaluate_datatree_complexity(self, component_names: List[str]) -> MetricResult:
        """è©•ä¼° DataTree è¤‡é›œåº¦"""
        config = self.metrics_config["datatree_complexity"]
        
        dt_keywords = ["flatten", "graft", "partition", "path mapper", "entwine"]
        dt_count = sum(1 for name in component_names if any(kw in name for kw in dt_keywords))
        
        if dt_count == 0:
            score = 1.0
            issues = []
            suggestions = []
        elif dt_count <= 2:
            score = 0.9
            issues = []
            suggestions = []
        elif dt_count <= 4:
            score = 0.7
            issues = [f"DataTree æ“ä½œè¼ƒå¤š ({dt_count} å€‹)"]
            suggestions = ["æª¢æŸ¥æ˜¯å¦å¯ä»¥ç°¡åŒ–æ•¸æ“šçµæ§‹"]
        else:
            score = 0.4
            issues = [f"DataTree æ“ä½œéå¤š ({dt_count} å€‹)"]
            suggestions = ["é‡æ–°è¨­è¨ˆæ•¸æ“šæµï¼Œæ¸›å°‘ DataTree æ“ä½œ"]
        
        return MetricResult(
            name="DataTree è¤‡é›œåº¦",
            category=config["category"],
            score=score,
            weight=config["weight"],
            description=f"{dt_count} å€‹ DataTree æ“ä½œ",
            issues=issues,
            suggestions=suggestions
        )
    
    def _evaluate_geometric_coupling(self, component_names: List[str]) -> MetricResult:
        """è©•ä¼°å¹¾ä½•è€¦åˆåº¦"""
        config = self.metrics_config["geometric_coupling"]
        
        # å¹¾ä½•é©…å‹•å…ƒä»¶
        geo_keywords = ["evaluate", "closest point", "surface cp", "point on", "curve cp"]
        geo_count = sum(1 for name in component_names if any(kw in name for kw in geo_keywords))
        
        # æ•¸å€¼ç¡¬ç·¨ç¢¼å…ƒä»¶
        num_keywords = ["number slider", "panel"]
        num_count = sum(1 for name in component_names if any(kw in name for kw in num_keywords))
        
        total = len(component_names)
        
        if total == 0:
            score = 0.5
        else:
            geo_ratio = geo_count / total
            num_ratio = num_count / total
            
            # å¹¾ä½•é©…å‹•æ¯”ä¾‹é«˜ï¼Œæ•¸å€¼ç¡¬ç·¨ç¢¼æ¯”ä¾‹ä½ = é«˜åˆ†
            score = 0.5 + geo_ratio * 0.4 - num_ratio * 0.1
            score = max(0.3, min(1.0, score))
        
        issues = []
        suggestions = []
        
        if geo_count == 0:
            suggestions.append("è€ƒæ…®ä½¿ç”¨ Evaluate Surface/Curve å¯¦ç¾å¹¾ä½•é©…å‹•è¨­è¨ˆ")
        
        return MetricResult(
            name="å¹¾ä½•è€¦åˆåº¦",
            category=config["category"],
            score=score,
            weight=config["weight"],
            description=f"{geo_count} å€‹å¹¾ä½•é©…å‹•å…ƒä»¶",
            issues=issues,
            suggestions=suggestions
        )
    
    def _evaluate_pattern_usage(
        self, 
        component_names: List[str], 
        patterns_matched: List[str]
    ) -> MetricResult:
        """è©•ä¼°å„ªé›…æ¨¡å¼ä½¿ç”¨"""
        config = self.metrics_config["pattern_usage"]
        
        # æª¢æŸ¥å„ªé›…å…ƒä»¶ä½¿ç”¨
        elegant_used = []
        for name in component_names:
            for elegant_name in ELEGANT_COMPONENTS:
                if elegant_name.lower() in name:
                    elegant_used.append(elegant_name)
        
        # è¨ˆç®—åˆ†æ•¸
        base_score = 0.5
        
        # æ¨¡å¼åŒ¹é…åŠ åˆ†
        pattern_bonus = len(patterns_matched) * 0.1
        
        # å„ªé›…å…ƒä»¶åŠ åˆ†
        elegant_bonus = sum(
            ELEGANT_COMPONENTS[e]["score_bonus"] 
            for e in elegant_used 
            if e in ELEGANT_COMPONENTS
        )
        
        score = min(1.0, base_score + pattern_bonus + elegant_bonus)
        
        suggestions = []
        if not elegant_used:
            suggestions.append("è€ƒæ…®ä½¿ç”¨ Graph Mapper æˆ– Remap Numbers å¯¦ç¾éç·šæ€§æ˜ å°„")
        
        return MetricResult(
            name="å„ªé›…æ¨¡å¼ä½¿ç”¨",
            category=config["category"],
            score=score,
            weight=config["weight"],
            description=f"åŒ¹é… {len(patterns_matched)} å€‹æ¨¡å¼, ä½¿ç”¨ {len(elegant_used)} å€‹å„ªé›…å…ƒä»¶",
            issues=[],
            suggestions=suggestions
        )
    
    def _evaluate_anti_patterns(
        self, 
        component_names: List[str],
        components: List[Dict],
        connections: List[Dict]
    ) -> MetricResult:
        """è©•ä¼°åæ¨¡å¼"""
        config = self.metrics_config["anti_pattern_absence"]
        
        issues = []
        suggestions = []
        penalty = 0.0
        
        # æª¢æŸ¥å·²çŸ¥åæ¨¡å¼
        for anti in ANTI_PATTERNS:
            if anti["detector"] and anti["detector"](component_names):
                issues.append(anti["description"])
                suggestions.append(anti["suggestion"])
                penalty += anti["penalty"]
        
        # æª¢æŸ¥åæ¨¡å¼å…ƒä»¶éåº¦ä½¿ç”¨
        for comp_name, rule in ANTI_PATTERN_COMPONENTS.items():
            count = sum(1 for name in component_names if comp_name.lower() in name)
            if count > rule["threshold"]:
                excess = count - rule["threshold"]
                penalty += excess * rule["penalty_per_excess"]
                issues.append(f"{comp_name} ä½¿ç”¨éå¤š ({count} å€‹)")
        
        score = max(0.3, 1.0 - penalty)
        
        return MetricResult(
            name="åæ¨¡å¼é¿å…",
            category=config["category"],
            score=score,
            weight=config["weight"],
            description=f"æª¢æ¸¬åˆ° {len(issues)} å€‹æ½›åœ¨å•é¡Œ",
            issues=issues,
            suggestions=suggestions
        )
    
    def _score_to_grade(self, score: float) -> str:
        """åˆ†æ•¸è½‰ç­‰ç´š"""
        if score >= 0.9:
            return "A"
        elif score >= 0.8:
            return "B"
        elif score >= 0.7:
            return "C"
        elif score >= 0.6:
            return "D"
        else:
            return "F"
    
    def _generate_summary(
        self, 
        metrics: List[MetricResult], 
        total_score: float, 
        grade: str
    ) -> str:
        """ç”Ÿæˆè©•ä¼°æ‘˜è¦"""
        all_issues = []
        all_suggestions = []
        
        for m in metrics:
            all_issues.extend(m.issues)
            all_suggestions.extend(m.suggestions)
        
        summary_parts = [
            f"ç¸½è©•åˆ†: {total_score:.2f} ({grade})",
        ]
        
        if all_issues:
            summary_parts.append(f"ç™¼ç¾ {len(all_issues)} å€‹å•é¡Œ")
        
        if grade in ["A", "B"]:
            summary_parts.append("æ–¹æ¡ˆå„ªé›…åº¦è‰¯å¥½")
        elif grade == "C":
            summary_parts.append("æ–¹æ¡ˆå¯æ¥å—ï¼Œä½†æœ‰æ”¹é€²ç©ºé–“")
        else:
            summary_parts.append("å»ºè­°é‡æ–°è¨­è¨ˆä»¥æé«˜å„ªé›…åº¦")
        
        return " | ".join(summary_parts)


# ============================================================
# ä¾¿åˆ©å‡½æ•¸
# ============================================================

def evaluate_gh_code(gh_code: Dict[str, Any]) -> EleganceReport:
    """è©•ä¼° GH Code çš„ä¾¿åˆ©å‡½æ•¸"""
    evaluator = EleganceEvaluator()
    return evaluator.evaluate(gh_code)


def quick_score(gh_code: Dict[str, Any]) -> float:
    """å¿«é€Ÿå–å¾—è©•åˆ†"""
    report = evaluate_gh_code(gh_code)
    return report.total_score


if __name__ == "__main__":
    # æ¸¬è©¦
    test_gh_code = {
        "components": [
            {"type": "Number Slider"},
            {"type": "Number Slider"},
            {"type": "Series"},
            {"type": "Sine"},
            {"type": "Cosine"},
            {"type": "Construct Point"},
            {"type": "Interpolate"},
            {"type": "Graph Mapper"},
        ],
        "connections": [
            {}, {}, {}, {}, {}, {}, {}
        ],
        "sliders": [
            {"name": "Turns"},
            {"name": "Radius"},
        ]
    }
    
    report = evaluate_gh_code(test_gh_code)
    
    print("=== Elegance Report ===")
    print(f"Grade: {report.grade}")
    print(f"Score: {report.total_score:.3f}")
    print(f"Summary: {report.summary}")
    print("\n--- Metrics ---")
    for m in report.metrics:
        print(f"  {m.name}: {m.score:.2f} (weight: {m.weight})")
        for issue in m.issues:
            print(f"    âš ï¸ {issue}")
        for sug in m.suggestions:
            print(f"    ğŸ’¡ {sug}")
