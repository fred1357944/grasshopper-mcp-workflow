#!/usr/bin/env python3
"""
Semantic Validator - èªžç¾©é©—è­‰å™¨
================================

è§£æ±º Pre-Execution Checker åªåšã€Œèªžæ³•é©—è­‰ã€çš„å•é¡Œã€‚
é€™å€‹æ¨¡çµ„è² è²¬é©—è­‰é…ç½®çš„ã€Œèªžç¾©æ­£ç¢ºæ€§ã€ï¼š

1. çµ„ä»¶è¡Œç‚ºè§£é‡‹ï¼šè§£é‡‹æ¯å€‹çµ„ä»¶å¯¦éš›åšä»€éº¼
2. è³‡æ–™æµåˆ†æžï¼šé ä¼°è¼¸å‡ºæ•¸é‡ï¼Œæª¢æ¸¬ explosion é¢¨éšª
3. åƒæ•¸èªžç¾©æª¢æŸ¥ï¼šSize vs Count, Radius vs Result ç­‰

æ ¸å¿ƒæ¦‚å¿µï¼š
    èªžæ³•æ­£ç¢º â‰  èªžç¾©æ­£ç¢º
    GUID æ­£ç¢º â‰  çµ„ä»¶ç”¨æ³•æ­£ç¢º
    é€£æŽ¥æˆåŠŸ â‰  è³‡æ–™æµåˆç†

2026-01-24
"""

import json
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional, Any, Set, Tuple
from enum import Enum


class SemanticRisk(Enum):
    """èªžç¾©é¢¨éšªç­‰ç´š"""
    NONE = "none"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"  # å¯èƒ½å°Žè‡´å´©æ½°


@dataclass
class ComponentBehavior:
    """çµ„ä»¶è¡Œç‚ºæè¿°"""
    name: str
    category: str
    description: str
    inputs: Dict[str, str]  # param_name -> description
    outputs: Dict[str, str]
    output_multiplier: str  # "1:1", "1:N", "N:M" ç­‰
    warnings: List[str] = field(default_factory=list)
    common_mistakes: List[str] = field(default_factory=list)


@dataclass
class DataFlowEstimate:
    """è³‡æ–™æµé ä¼°"""
    component_id: str
    component_type: str
    input_count: int  # é ä¼°è¼¸å…¥æ•¸é‡
    output_count: int  # é ä¼°è¼¸å‡ºæ•¸é‡
    multiplier_reason: str  # ç‚ºä»€éº¼æœ‰é€™å€‹ä¹˜æ•¸
    risk_level: SemanticRisk
    warning: Optional[str] = None


@dataclass
class SemanticCheckResult:
    """èªžç¾©æª¢æŸ¥çµæžœ"""
    passed: bool
    component_id: str
    check_type: str  # "behavior" | "dataflow" | "parameter"
    risk_level: SemanticRisk
    message: str
    explanation: Optional[str] = None
    suggestion: Optional[str] = None


from grasshopper_mcp.knowledge_base import ConnectionKnowledgeBase

class SemanticValidator:
    """
    èªžç¾©é©—è­‰å™¨

    ä½¿ç”¨æ–¹å¼ï¼š
    ```python
    validator = SemanticValidator()
    results = validator.validate(placement_info)
    report = validator.generate_human_readable_report()
    ```
    """

    # çµ„ä»¶è¡Œç‚ºçŸ¥è­˜åº«
    COMPONENT_BEHAVIORS: Dict[str, ComponentBehavior] = {
        "Mesh Box": ComponentBehavior(
            name="Mesh Box",
            category="Mesh/Primitive",
            description="å‰µå»ºä¸€å€‹ Mesh ç«‹æ–¹é«”ï¼ŒX/Y/Z åƒæ•¸æ˜¯ã€Œç´°åˆ†æ•¸é‡ã€è€Œéžå°ºå¯¸",
            inputs={
                "X": "X æ–¹å‘ç´°åˆ†æ•¸é‡ (éžå°ºå¯¸ï¼)",
                "Y": "Y æ–¹å‘ç´°åˆ†æ•¸é‡ (éžå°ºå¯¸ï¼)",
                "Z": "Z æ–¹å‘ç´°åˆ†æ•¸é‡ (éžå°ºå¯¸ï¼)"
            },
            outputs={"M": "è¼¸å‡º Meshï¼ŒåŒ…å« X*Y*Z å€‹é¢"},
            output_multiplier="1:X*Y*Z",
            warnings=["X/Y/Z æ˜¯ç´°åˆ†æ•¸ï¼Œä¸æ˜¯å°ºå¯¸ï¼10Ã—10Ã—10 = 1000 å€‹é¢"],
            common_mistakes=["èª¤ä»¥ç‚º X/Y/Z æ˜¯å°ºå¯¸ï¼Œå°Žè‡´é¢æ•¸çˆ†ç‚¸"]
        ),
        "Center Box": ComponentBehavior(
            name="Center Box",
            category="Surface/Primitive",
            description="å‰µå»ºä¸€å€‹ä»¥åŽŸé»žç‚ºä¸­å¿ƒçš„ Brep ç«‹æ–¹é«”ï¼ŒX/Y/Z æ˜¯çœŸæ­£çš„å°ºå¯¸",
            inputs={
                "B": "åŸºæº–å¹³é¢",
                "X": "X æ–¹å‘å°ºå¯¸",
                "Y": "Y æ–¹å‘å°ºå¯¸",
                "Z": "Z æ–¹å‘å°ºå¯¸"
            },
            outputs={"B": "è¼¸å‡º Brepï¼Œå–®ä¸€ç«‹æ–¹é«”"},
            output_multiplier="1:1",
            warnings=[],
            common_mistakes=[]
        ),
        "Face Normals": ComponentBehavior(
            name="Face Normals",
            category="Mesh/Analysis",
            description="è¨ˆç®— Mesh æ¯å€‹é¢çš„ä¸­å¿ƒé»žå’Œæ³•å‘é‡",
            inputs={"M": "è¼¸å…¥ Mesh"},
            outputs={
                "C": "é¢ä¸­å¿ƒé»ž (èˆ‡é¢æ•¸ç›¸åŒ)",
                "N": "é¢æ³•å‘é‡ (èˆ‡é¢æ•¸ç›¸åŒ)"
            },
            output_multiplier="1:N (N = è¼¸å…¥ Mesh é¢æ•¸)",
            warnings=["è¼¸å‡ºæ•¸é‡ = è¼¸å…¥ Mesh é¢æ•¸"],
            common_mistakes=["ä½¿ç”¨é«˜ç´°åˆ† Mesh å°Žè‡´è¼¸å‡ºçˆ†ç‚¸"]
        ),
        "Deconstruct Brep": ComponentBehavior(
            name="Deconstruct Brep",
            category="Surface/Analysis",
            description="åˆ†è§£ Brep ç‚ºé¢ã€é‚Šã€é ‚é»ž",
            inputs={"B": "è¼¸å…¥ Brep"},
            outputs={
                "F": "é¢åˆ—è¡¨",
                "E": "é‚Šåˆ—è¡¨",
                "V": "é ‚é»žåˆ—è¡¨"
            },
            output_multiplier="1:å›ºå®š (å–æ±ºæ–¼ Brep è¤‡é›œåº¦)",
            warnings=["ç«‹æ–¹é«”è¼¸å‡º 6 å€‹é¢"],
            common_mistakes=[]
        ),
        "Evaluate Surface": ComponentBehavior(
            name="Evaluate Surface",
            category="Surface/Analysis",
            description="åœ¨è¡¨é¢ UV åº§æ¨™è™•è©•ä¼°ï¼Œç²å–é»žã€æ³•å‘é‡ã€Frame",
            inputs={
                "S": "è¼¸å…¥ Surface",
                "U": "U åƒæ•¸ (0-1)",
                "V": "V åƒæ•¸ (0-1)"
            },
            outputs={
                "P": "é»ž",
                "N": "æ³•å‘é‡",
                "F": "Frame (Plane)"
            },
            output_multiplier="1:1",
            warnings=["UV=0.5,0.5 æ˜¯è¡¨é¢ä¸­å¿ƒ"],
            common_mistakes=[]
        ),
        "Wasp_Connection From Direction": ComponentBehavior(
            name="Wasp_Connection From Direction",
            category="WASP/Connection",
            description="å¾ž Geometryã€Centerã€Up æ–¹å‘å‰µå»º WASP é€£æŽ¥é»ž",
            inputs={
                "GEO": "å¹¾ä½• (å¿…é ˆæ˜¯ Mesh!)",
                "CEN": "é€£æŽ¥é»žä¸­å¿ƒ",
                "UP": "å‘ä¸Šæ–¹å‘ (Line, éž Vector)"
            },
            outputs={"CONN": "é€£æŽ¥é»žåˆ—è¡¨"},
            output_multiplier="1:N (N = CEN è¼¸å…¥æ•¸é‡)",
            warnings=["GEO å¿…é ˆæ˜¯ Meshï¼ŒUP å¿…é ˆæ˜¯ Line"],
            common_mistakes=["å‚³å…¥ Brep è€Œéž Mesh", "å‚³å…¥ Vector è€Œéž Line"]
        ),
        "Wasp_Connection From Plane": ComponentBehavior(
            name="Wasp_Connection From Plane",
            category="WASP/Connection",
            description="å¾ž Geometry å’Œ Plane å‰µå»º WASP é€£æŽ¥é»ž (æŽ¨è–¦ç”¨æ³•)",
            inputs={
                "GEO": "å¹¾ä½•",
                "PLN": "é€£æŽ¥å¹³é¢"
            },
            outputs={"CONN": "é€£æŽ¥é»žåˆ—è¡¨"},
            output_multiplier="1:N (N = PLN è¼¸å…¥æ•¸é‡)",
            warnings=[],
            common_mistakes=[]
        ),
        "Wasp_Basic Part": ComponentBehavior(
            name="Wasp_Basic Part",
            category="WASP/Part",
            description="å‰µå»º WASP Part",
            inputs={
                "NAME": "Part åç¨±",
                "GEO": "å¹¾ä½•",
                "CONN": "é€£æŽ¥é»žåˆ—è¡¨"
            },
            outputs={"PART": "WASP Part"},
            output_multiplier="1:1",
            warnings=[],
            common_mistakes=[]
        ),
        "Wasp_Rules Generator": ComponentBehavior(
            name="Wasp_Rules Generator",
            category="WASP/Rule",
            description="è‡ªå‹•ç”Ÿæˆ WASP è¦å‰‡ (æŽ¨è–¦ï¼åªéœ€ PART è¼¸å…¥)",
            inputs={"PART": "Part åˆ—è¡¨"},
            outputs={"R": "è¦å‰‡åˆ—è¡¨"},
            output_multiplier="1:NÃ—N (å…¨é€£æŽ¥)",
            warnings=["è¼¸å‡ºæ•¸é‡ = Part æ•¸é‡ Ã— é€£æŽ¥é»žæ•¸é‡"],
            common_mistakes=["èˆ‡ Wasp_Rule æ··æ·†"]
        ),
        "Wasp_Rule": ComponentBehavior(
            name="Wasp_Rule",
            category="WASP/Rule",
            description="æ‰‹å‹•å®šç¾©å–®ä¸€ WASP è¦å‰‡ (éœ€è¦ P1/C1/P2/C2)",
            inputs={
                "P1": "Part 1",
                "C1": "Connection 1",
                "P2": "Part 2",
                "C2": "Connection 2"
            },
            outputs={"R": "å–®ä¸€è¦å‰‡"},
            output_multiplier="1:1",
            warnings=["éœ€è¦æ‰‹å‹•å®šç¾©æ¯å€‹é€£æŽ¥ï¼Œè¼ƒç¹ç‘£"],
            common_mistakes=["ä½¿ç”¨ Rule è€Œéž Rules Generator"]
        ),
        "Wasp_Stochastic Aggregation": ComponentBehavior(
            name="Wasp_Stochastic Aggregation",
            category="WASP/Aggregation",
            description="éš¨æ©Ÿèšé›†",
            inputs={
                "PART": "Part åˆ—è¡¨",
                "RULES": "è¦å‰‡åˆ—è¡¨",
                "N": "èšé›†æ•¸é‡",
                "SEED": "éš¨æ©Ÿç¨®å­",
                "RESET": "é‡ç½® (Boolean Toggle!)"
            },
            outputs={
                "AGG": "èšé›†ç‰©ä»¶",
                "GEO": "å¹¾ä½•åˆ—è¡¨"
            },
            output_multiplier="1:N",
            warnings=["RESET å¿…é ˆé€£æŽ¥ Boolean Toggle"],
            common_mistakes=["RESET æœªé€£æŽ¥å°Žè‡´ç„¡æ³•é‡ç½®"]
        )
    }

    # è³‡æ–™æµçˆ†ç‚¸è­¦æˆ’å€¼
    OUTPUT_EXPLOSION_THRESHOLD = 100

    def __init__(self, config_dir: Optional[Path] = None):
        self.results: List[SemanticCheckResult] = []
        self.data_flow_estimates: List[DataFlowEstimate] = []
        
        # Load Knowledge Base
        if config_dir is None:
            possible_paths = [
                Path(__file__).parent.parent / "config",
                Path.cwd() / "config",
            ]
            for p in possible_paths:
                if p.exists():
                    config_dir = p
                    break
            else:
                config_dir = possible_paths[0]
        self.kb = ConnectionKnowledgeBase(storage_dir=config_dir)

    def validate(self, placement_info: Dict) -> List[SemanticCheckResult]:
        """åŸ·è¡Œèªžç¾©é©—è­‰"""
        self.results = []
        self.data_flow_estimates = []

        components = placement_info.get("components", [])
        connections = placement_info.get("connections", [])

        # 1. æª¢æŸ¥æ¯å€‹çµ„ä»¶çš„è¡Œç‚º
        for comp in components:
            self._check_component_behavior(comp)

        # 2. åˆ†æžè³‡æ–™æµ
        self._analyze_data_flow(components, connections)

        # 3. æª¢æŸ¥ç‰¹å®šæ¨¡å¼çš„é¢¨éšª
        self._check_pattern_risks(components, connections)
        
        # 4. æª¢æŸ¥é€£æŽ¥ä¿¡å¿ƒåº¦ (KB Based)
        self._check_connection_confidence(components, connections)

        return self.results

    def _check_connection_confidence(self, components: List[Dict], connections: List[Dict]):
        """ä½¿ç”¨çŸ¥è­˜åº«æª¢æŸ¥é€£æŽ¥çš„ä¿¡å¿ƒåº¦ (çµ±è¨ˆå­¸é©—è­‰)"""
        comp_lookup = {c.get("id"): c for c in components}
        
        for conn in connections:
            from_id = conn.get("from")
            to_id = conn.get("to")
            from_param = conn.get("fromParam")
            to_param = conn.get("toParam")
            
            src_comp = comp_lookup.get(from_id)
            tgt_comp = comp_lookup.get(to_id)
            
            if src_comp and tgt_comp:
                confidence = self.kb.get_connection_confidence(
                    src_comp.get("type", ""), from_param,
                    tgt_comp.get("type", ""), to_param
                )
                
                # å¦‚æžœä¿¡å¿ƒåº¦éŽä½Ž (ä¸”ä¸æ˜¯ç¬¬ä¸€æ¬¡é‡åˆ°çš„çµ„ä»¶)
                # é€™è£¡å‡è¨­å¦‚æžœ KB æ˜¯ç©ºçš„ï¼Œconfidence æœƒæ˜¯ 0ï¼Œæˆ‘å€‘ä¸å¸Œæœ›å…¨éƒ¨å ±éŒ¯ã€‚
                # æ‰€ä»¥å¯ä»¥åŠ ä¸€å€‹é–¾å€¼ï¼šå¦‚æžœ KB ä¸­æœ‰è©²çµ„ä»¶çš„å…¶ä»–é€£æŽ¥è¨˜éŒ„ï¼Œä½†æ²’æœ‰é€™å€‹ç‰¹å®šé€£æŽ¥ï¼Œå‰‡å ±è­¦ã€‚
                
                if confidence == 0.0:
                    # é€™æ˜¯ä¸€å€‹æœªè¦‹éŽçš„é€£æŽ¥
                    # æˆ‘å€‘å°‡å…¶æ¨™è¨˜ç‚º Low Risk (Info)ï¼Œæé†’ç”¨æˆ¶é€™æ˜¯ä¸€å€‹æ–°ç©Žçš„ç”¨æ³•
                    self.results.append(SemanticCheckResult(
                        passed=True,
                        component_id=from_id,
                        check_type="confidence",
                        risk_level=SemanticRisk.LOW,
                        message=f"New Pattern: {src_comp.get('type')} -> {tgt_comp.get('type')}",
                        explanation=f"This connection pattern ({from_param}->{to_param}) has not been recorded in the Knowledge Base yet.",
                        suggestion="If this works, it will be added to the KB automatically."
                    ))

    def _check_component_behavior(self, component: Dict):
        """æª¢æŸ¥å–®ä¸€çµ„ä»¶çš„è¡Œç‚ºæ˜¯å¦ç¬¦åˆé æœŸ"""
        comp_type = component.get("type", "")
        comp_id = component.get("nickname", component.get("id", "Unknown"))

        behavior = self.COMPONENT_BEHAVIORS.get(comp_type)
        if not behavior:
            return  # æœªçŸ¥çµ„ä»¶ï¼Œè·³éŽ

        # æª¢æŸ¥å¸¸è¦‹éŒ¯èª¤
        for mistake in behavior.common_mistakes:
            self.results.append(SemanticCheckResult(
                passed=True,  # åªæ˜¯è­¦å‘Šï¼Œä¸é˜»æ“‹
                component_id=comp_id,
                check_type="behavior",
                risk_level=SemanticRisk.MEDIUM,
                message=f"âš ï¸ {comp_type}: æ³¨æ„å¸¸è¦‹éŒ¯èª¤",
                explanation=mistake,
                suggestion=f"è«‹ç¢ºèªé€™æ˜¯ä½ æƒ³è¦çš„è¡Œç‚º"
            ))

        # æª¢æŸ¥è­¦å‘Š
        for warning in behavior.warnings:
            self.results.append(SemanticCheckResult(
                passed=True,
                component_id=comp_id,
                check_type="behavior",
                risk_level=SemanticRisk.LOW,
                message=f"â„¹ï¸ {comp_type}: {warning}",
                explanation=behavior.description
            ))

    def _analyze_data_flow(self, components: List[Dict], connections: List[Dict]):
        """åˆ†æžè³‡æ–™æµï¼Œé ä¼°è¼¸å‡ºæ•¸é‡"""
        # å»ºç«‹çµ„ä»¶æŸ¥æ‰¾è¡¨
        comp_by_nickname = {c.get("nickname", c.get("id")): c for c in components}

        # å°‹æ‰¾æ½›åœ¨çš„è³‡æ–™çˆ†ç‚¸
        for comp in components:
            comp_type = comp.get("type", "")
            comp_id = comp.get("nickname", comp.get("id", "Unknown"))
            behavior = self.COMPONENT_BEHAVIORS.get(comp_type)

            if not behavior:
                continue

            # ç‰¹æ®Šæª¢æŸ¥ï¼šMesh Box
            if comp_type == "Mesh Box":
                props = comp.get("properties", {})
                x = props.get("value", 10) if comp.get("nickname") == "SizeX" else 10
                y = props.get("value", 10) if comp.get("nickname") == "SizeY" else 10
                z = props.get("value", 10) if comp.get("nickname") == "SizeZ" else 10

                # æŸ¥æ‰¾å¯¦éš›çš„ slider å€¼
                for c in components:
                    if c.get("nickname") == "SizeX":
                        x = c.get("properties", {}).get("value", 10)
                    elif c.get("nickname") == "SizeY":
                        y = c.get("properties", {}).get("value", 10)
                    elif c.get("nickname") == "SizeZ":
                        z = c.get("properties", {}).get("value", 10)

                estimated_faces = x * y * z * 6  # 6 å€‹æ–¹å‘çš„é¢

                if estimated_faces > self.OUTPUT_EXPLOSION_THRESHOLD:
                    self.results.append(SemanticCheckResult(
                        passed=False,
                        component_id=comp_id,
                        check_type="dataflow",
                        risk_level=SemanticRisk.CRITICAL,
                        message=f"ðŸ”´ è³‡æ–™æµçˆ†ç‚¸é¢¨éšªï¼",
                        explanation=f"Mesh Box è¨­å®š X={x}, Y={y}, Z={z} å°‡ç”¢ç”Ÿç´„ {estimated_faces} å€‹ mesh faces",
                        suggestion="å¦‚æžœä½ æƒ³è¦çš„æ˜¯ä¸€å€‹ç°¡å–®ç«‹æ–¹é«”ï¼Œè«‹ä½¿ç”¨ Center Box æ›¿ä»£"
                    ))

                self.data_flow_estimates.append(DataFlowEstimate(
                    component_id=comp_id,
                    component_type=comp_type,
                    input_count=3,
                    output_count=estimated_faces,
                    multiplier_reason=f"X*Y*Z*6 = {x}*{y}*{z}*6",
                    risk_level=SemanticRisk.CRITICAL if estimated_faces > 100 else SemanticRisk.LOW,
                    warning=f"å°‡ç”¢ç”Ÿ {estimated_faces} å€‹é¢" if estimated_faces > 10 else None
                ))

    def _check_pattern_risks(self, components: List[Dict], connections: List[Dict]):
        """æª¢æŸ¥ç‰¹å®šæ¨¡å¼çš„é¢¨éšª"""
        comp_types = {c.get("type", "") for c in components}

        # æª¢æŸ¥ WASP æ¨¡å¼
        if "Wasp_Stochastic Aggregation" in comp_types:
            # æª¢æŸ¥ RESET æ˜¯å¦é€£æŽ¥
            reset_connected = any(
                conn.get("to_param") == "RESET"
                for conn in connections
            )
            if not reset_connected:
                self.results.append(SemanticCheckResult(
                    passed=False,
                    component_id="StochAggr",
                    check_type="pattern",
                    risk_level=SemanticRisk.HIGH,
                    message="âš ï¸ WASP Stochastic Aggregation çš„ RESET æœªé€£æŽ¥",
                    explanation="RESET è¼¸å…¥å¿…é ˆé€£æŽ¥ Boolean Toggleï¼Œå¦å‰‡ç„¡æ³•é‡ç½®èšé›†",
                    suggestion="æ·»åŠ  Boolean Toggle ä¸¦é€£æŽ¥åˆ° RESET"
                ))

            # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨ Mesh Box è€Œéž Center Box
            if "Mesh Box" in comp_types and "Center Box" not in comp_types:
                self.results.append(SemanticCheckResult(
                    passed=False,
                    component_id="MeshBox",
                    check_type="pattern",
                    risk_level=SemanticRisk.CRITICAL,
                    message="ðŸ”´ WASP é…ç½®å¯èƒ½éŒ¯èª¤ï¼šä½¿ç”¨ Mesh Box è€Œéž Center Box",
                    explanation="WASP é€šå¸¸ä½¿ç”¨ Center Box (Brep) ä½œç‚º Part å¹¾ä½•ï¼Œè€Œéž Mesh Box",
                    suggestion="ä½¿ç”¨ Center Box + Deconstruct Brep + Evaluate Surface æ›¿ä»£"
                ))

        # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨ Rule è€Œéž Rules Generator
        if "Wasp_Rule" in comp_types and "Wasp_Rules Generator" not in comp_types:
            self.results.append(SemanticCheckResult(
                passed=True,
                component_id="Rule",
                check_type="pattern",
                risk_level=SemanticRisk.MEDIUM,
                message="âš ï¸ ä½¿ç”¨æ‰‹å‹• Rule è€Œéž Rules Generator",
                explanation="Rules Generator åªéœ€ PART è¼¸å…¥ï¼Œæœƒè‡ªå‹•ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„è¦å‰‡",
                suggestion="è€ƒæ…®ä½¿ç”¨ Wasp_Rules Generator ç°¡åŒ–é…ç½®"
            ))

    def generate_human_readable_report(self) -> str:
        """ç”Ÿæˆäººé¡žå¯è®€çš„èªžç¾©é©—è­‰å ±å‘Š"""
        lines = ["## ðŸ§  èªžç¾©é©—è­‰å ±å‘Š\n"]

        critical = [r for r in self.results if r.risk_level == SemanticRisk.CRITICAL]
        high = [r for r in self.results if r.risk_level == SemanticRisk.HIGH]
        medium = [r for r in self.results if r.risk_level == SemanticRisk.MEDIUM]

        if critical:
            lines.append("### ðŸ”´ Critical (é˜»æ“‹åŸ·è¡Œ)")
            for r in critical:
                lines.append(f"- **{r.component_id}**: {r.message}")
                if r.explanation:
                    lines.append(f"  - èªªæ˜Ž: {r.explanation}")
                if r.suggestion:
                    lines.append(f"  - å»ºè­°: {r.suggestion}")
            lines.append("")

        if high:
            lines.append("### ðŸŸ  High Risk")
            for r in high:
                lines.append(f"- **{r.component_id}**: {r.message}")
                if r.suggestion:
                    lines.append(f"  â†’ {r.suggestion}")
            lines.append("")

        if medium:
            lines.append("### ðŸŸ¡ Medium (è«‹ç¢ºèª)")
            for r in medium:
                lines.append(f"- **{r.component_id}**: {r.message}")
            lines.append("")

        # è³‡æ–™æµæ‘˜è¦
        if self.data_flow_estimates:
            lines.append("### ðŸ“Š è³‡æ–™æµé ä¼°")
            for est in self.data_flow_estimates:
                if est.warning:
                    lines.append(f"- **{est.component_id}** ({est.component_type}): {est.warning}")
            lines.append("")

        # çµè«–
        if critical:
            lines.append("### çµè«–: âŒ ä¸é€šéŽ - è«‹ä¿®å¾© Critical å•é¡Œ")
        elif high:
            lines.append("### çµè«–: âš ï¸ æœ‰é¢¨éšª - å»ºè­°è™•ç† High Risk å•é¡Œ")
        else:
            lines.append("### çµè«–: âœ… é€šéŽ")

        return "\n".join(lines)

    def get_component_explanation(self, comp_type: str) -> Optional[str]:
        """ç²å–çµ„ä»¶çš„äººé¡žå¯è®€è§£é‡‹"""
        behavior = self.COMPONENT_BEHAVIORS.get(comp_type)
        if not behavior:
            return None

        lines = [
            f"**{behavior.name}** ({behavior.category})",
            f"",
            f"{behavior.description}",
            f"",
            f"**è¼¸å…¥:**"
        ]
        for param, desc in behavior.inputs.items():
            lines.append(f"  - `{param}`: {desc}")

        lines.append(f"")
        lines.append(f"**è¼¸å‡º:**")
        for param, desc in behavior.outputs.items():
            lines.append(f"  - `{param}`: {desc}")

        lines.append(f"")
        lines.append(f"**è¼¸å‡ºä¹˜æ•¸:** {behavior.output_multiplier}")

        if behavior.warnings:
            lines.append(f"")
            lines.append(f"**âš ï¸ æ³¨æ„:**")
            for w in behavior.warnings:
                lines.append(f"  - {w}")

        return "\n".join(lines)


def validate_placement_info(json_path: str) -> str:
    """ä¾¿æ·å‡½æ•¸ï¼šé©—è­‰ placement_info.json ä¸¦è¿”å›žå ±å‘Š"""
    with open(json_path, 'r', encoding='utf-8') as f:
        config = json.load(f)

    validator = SemanticValidator()
    validator.validate(config)
    return validator.generate_human_readable_report()


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        report = validate_placement_info(sys.argv[1])
        print(report)
    else:
        print("Usage: python semantic_validator.py <placement_info.json>")
